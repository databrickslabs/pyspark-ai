{"key": "You are an Apache Spark SQL expert, who can summary what a dataframe retrieves. Given an analyzed \nquery plan of a dataframe, you will \n1. convert the dataframe to SQL query. Note that an explain output contains plan \nnodes separated by `\\n`. Each plan node has its own expressions and expression ids. \n2. summary what the sql query retrieves. \n\n\n\nanalyzed_plan: \nGlobalLimit 100\n    +- LocalLimit 100\n       +- Sort [d_year ASC NULLS FIRST, sum_agg DESC NULLS LAST, brand_id ASC NULLS FIRST], true\n          +- Aggregate [d_year, i_brand, i_brand_id], [d_year, i_brand_id AS brand_id, i_brand AS brand, sum(ss_ext_sales_price) AS sum_agg]\n             +- Filter (((d_date_sk = ss_sold_date_sk) AND (ss_item_sk = i_item_sk)) AND ((i_manufact_id = 128) AND (d_moy = 11)))\n                +- Join Inner\n                   :- Join Inner\n                   :  :- SubqueryAlias dt\n                   :  :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.date_dim\n                   :  :     +- Relation spark_catalog.tpcds_sf1_delta.date_dim[d_date_sk,d_date_id,d_date,d_month_seq,d_week_seq,d_quarter_seq,d_year,d_dow,d_moy,d_dom,d_qoy,d_fy_year,d_fy_quarter_seq,d_fy_week_seq,d_day_name,d_quarter_name,d_holiday,d_weekend,d_following_holiday,d_first_dom,d_last_dom,d_same_day_ly,d_same_day_lq,d_current_day,... 4 more fields] parquet\n                   :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.store_sales\n                   :     +- Relation spark_catalog.tpcds_sf1_delta.store_sales[ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk,ss_cdemo_sk,ss_hdemo_sk,ss_addr_sk,ss_store_sk,ss_promo_sk,ss_ticket_numberL,ss_quantity,ss_wholesale_cost,ss_list_price,ss_sales_price,ss_ext_discount_amt,ss_ext_sales_price,ss_ext_wholesale_cost,ss_ext_list_price,ss_ext_tax,ss_coupon_amt,ss_net_paid,ss_net_paid_inc_tax,ss_net_profit] parquet\n                   +- SubqueryAlias spark_catalog.tpcds_sf1_delta.item\n                      +- Relation spark_catalog.tpcds_sf1_delta.item[i_item_sk,i_item_id,i_rec_start_date,i_rec_end_date,i_item_desc,i_current_price,i_wholesale_cost,i_brand_id,i_brand,i_class_id,i_class,i_category_id,i_category,i_manufact_id,i_manufact,i_size,i_formulation,i_color,i_units,i_container,i_manager_id,i_product_name] parquet\n\nexplain: \nThe analyzed plan can be translated into the following SQL query:\n```sql\nSELECT\n  dt.d_year,\n  item.i_brand_id brand_id,\n  item.i_brand brand,\n  SUM(ss_ext_sales_price) sum_agg\nFROM date_dim dt, store_sales, item\nWHERE dt.d_date_sk = store_sales.ss_sold_date_sk\n  AND store_sales.ss_item_sk = item.i_item_sk\n  AND item.i_manufact_id = 128\n  AND dt.d_moy = 11\nGROUP BY dt.d_year, item.i_brand, item.i_brand_id\nORDER BY dt.d_year, sum_agg DESC, brand_id\nLIMIT 100\n```\nIn summary, this dataframe is retrieving the top 100 brands (specifically of items manufactured by manufacturer with id 128) with the highest total sales price for each year in the month of November. It presents the results sorted by year, total sales (in descending order), and brand id.\n\n\n\nanalyzed_plan: SubqueryAlias auto_sales_2022\n+- View (`auto_sales_2022`, [brand,us_sales,sales_change])\n   +- Project [cast(brand as string) AS brand, cast(us_sales as int) AS us_sales, cast(sales_change as int) AS sales_change]\n      +- Project [brand, us_sales, sales_change]\n         +- SubqueryAlias v1\n            +- LocalRelation [brand, us_sales, sales_change]\n\nexplain:", "value": "The analyzed plan can be translated into the following SQL query:\n```sql\nCREATE VIEW auto_sales_2022 AS\nSELECT\n  CAST(brand AS STRING) AS brand,\n  CAST(us_sales AS INT) AS us_sales,\n  CAST(sales_change AS INT) AS sales_change\nFROM v1\n```\n\nIn summary, this dataframe is creating a view called `auto_sales_2022` based on a local relation `v1`. The view consists of three columns: brand, us_sales, and sales_change, where the data types are converted to string, int, and int, respectively."}
